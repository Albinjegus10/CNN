{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <a id=\"2\">Load packages</a>"
      ],
      "metadata": {
        "_uuid": "5a708dc52b2e5990e2247ed573d50d0f6933b730",
        "id": "OFztjxQMjMhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.python import keras\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "_uuid": "2defa674e4e6d0e7371df92e7d1f388fc5c14bb6",
        "execution": {
          "iopub.status.busy": "2023-05-01T12:14:44.246605Z",
          "iopub.execute_input": "2023-05-01T12:14:44.246954Z",
          "iopub.status.idle": "2023-05-01T12:14:44.255716Z",
          "shell.execute_reply.started": "2023-05-01T12:14:44.246902Z",
          "shell.execute_reply": "2023-05-01T12:14:44.254575Z"
        },
        "trusted": true,
        "id": "jMaBTrRRjMhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "_uuid": "e17fd2539412442ddb3ecacf84366ddd62faf836",
        "id": "sNyacOlajMhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_ROWS = 28\n",
        "IMG_COLS = 28\n",
        "NUM_CLASSES = 10\n",
        "TEST_SIZE = 0.1\n",
        "RANDOM_STATE = 2018\n",
        "#Model\n",
        "NO_EPOCHS = 150\n",
        "PATIENCE = 20\n",
        "VERBOSE = 1\n",
        "BATCH_SIZE = 128\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "d7a44bfc7a28df7026241c4a7e047298446f1554",
        "execution": {
          "iopub.status.busy": "2023-05-01T11:38:11.922719Z",
          "iopub.execute_input": "2023-05-01T11:38:11.923327Z",
          "iopub.status.idle": "2023-05-01T11:38:11.930288Z",
          "shell.execute_reply.started": "2023-05-01T11:38:11.923273Z",
          "shell.execute_reply": "2023-05-01T11:38:11.929331Z"
        },
        "trusted": true,
        "id": "zR9Tzs9QjMhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df = pd.read_csv(\"/content/sample_data/mnist_train.csv\")\n",
        "test_df = pd.read_csv(\"/content/sample_data/mnist_test.csv\")"
      ],
      "metadata": {
        "_uuid": "a9c3148fda056ecc88570b302f5185064d5e9fc8",
        "execution": {
          "iopub.status.busy": "2023-05-01T12:21:52.448039Z",
          "iopub.execute_input": "2023-05-01T12:21:52.448627Z",
          "iopub.status.idle": "2023-05-01T12:22:00.000978Z",
          "shell.execute_reply.started": "2023-05-01T12:21:52.448563Z",
          "shell.execute_reply": "2023-05-01T12:21:59.999974Z"
        },
        "trusted": true,
        "id": "bEqXfMJNjMhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MNIST train -  rows:\",train_df.shape[0],\" columns:\", train_df.shape[1])\n",
        "print(\"MNIST test -  rows:\",test_df.shape[0],\" columns:\", test_df.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qko0cDwRmxW5",
        "outputId": "fb271e54-4d37-43fb-db50-88fce48c5ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST train -  rows: 29812  columns: 785\n",
            "MNIST test -  rows: 10000  columns: 785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a id=\"4\">Data exploration</a>"
      ],
      "metadata": {
        "_uuid": "290f8f38b2f64dfd1bd3432eb1e9a101dbbaf4e5",
        "id": "9soSFk_QjMhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](http://)The dimmension of the original  train,  test set are as following:"
      ],
      "metadata": {
        "_uuid": "e8891ae7996aa2ae5bc1c025661a77ae91c2fdfb",
        "id": "-4yjTvpNjMhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MNIST train -  rows:\",train_df.shape[0],\" columns:\", train_df.shape[1])\n",
        "print(\"MNIST test -  rows:\",test_df.shape[0],\" columns:\", test_df.shape[1])"
      ],
      "metadata": {
        "_uuid": "a133496cefc53baa56b9a5eec93bfd064ea6360d",
        "execution": {
          "iopub.status.busy": "2023-05-01T12:22:10.346292Z",
          "iopub.execute_input": "2023-05-01T12:22:10.346695Z",
          "iopub.status.idle": "2023-05-01T12:22:10.355023Z",
          "shell.execute_reply.started": "2023-05-01T12:22:10.346627Z",
          "shell.execute_reply": "2023-05-01T12:22:10.353942Z"
        },
        "trusted": true,
        "id": "G9ZAyvBrjMhl",
        "outputId": "e0dfcc27-6286-47d6-976c-803eadf1e169"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "MNIST train -  rows: 42000  columns: 785\nMNIST test -  rows: 28000  columns: 784\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a id=\"51\">Prepare the model</a>\n",
        "\n",
        "## Data preprocessing\n",
        "\n",
        "First we will do a data preprocessing to prepare for the model.\n",
        "\n",
        "We reshape the columns  from (784) to (28,28,1). We also save label (target) feature as a separate vector."
      ],
      "metadata": {
        "_uuid": "10b920b2ffc5de4a3ddebe494966d90cca91ee95",
        "id": "J-fNkW7IjMhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "def data_preprocessing(raw, hasLabel=True):\n",
        "    start_pixel = 0\n",
        "    if(hasLabel):\n",
        "        start_pixel = 1\n",
        "    if(hasLabel):\n",
        "        out_y = keras.utils.to_categorical(raw.label, NUM_CLASSES)\n",
        "    else:\n",
        "        out_y = None\n",
        "    num_images = raw.shape[0]\n",
        "    x_as_array = raw.values[:,start_pixel:]\n",
        "    x_shaped_array = x_as_array.reshape(num_images, IMG_ROWS, IMG_COLS, 1)\n",
        "    out_x = x_shaped_array / 255\n",
        "    return out_x, out_y"
      ],
      "metadata": {
        "_uuid": "db4c308b9fb334a54b4c055b00d2a1fbcf77ab96",
        "execution": {
          "iopub.status.busy": "2023-05-01T12:25:09.817512Z",
          "iopub.execute_input": "2023-05-01T12:25:09.818228Z",
          "iopub.status.idle": "2023-05-01T12:25:09.826908Z",
          "shell.execute_reply.started": "2023-05-01T12:25:09.817937Z",
          "shell.execute_reply": "2023-05-01T12:25:09.825605Z"
        },
        "trusted": true,
        "id": "njI77hb1jMhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We process both the train_data and the test_data"
      ],
      "metadata": {
        "_uuid": "2d3cafa55173d40cd9a42df63d4919f03b264c09",
        "id": "0gIkz1vMjMhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the data\n",
        "X, y = data_preprocessing(train_df)\n",
        "X_test, y_test = data_preprocessing(test_df,hasLabel=False)"
      ],
      "metadata": {
        "_uuid": "454d7a8fdca4bdbefc04a9d796de04b6af4a1767",
        "execution": {
          "iopub.status.busy": "2023-05-01T12:25:31.157652Z",
          "iopub.execute_input": "2023-05-01T12:25:31.157978Z",
          "iopub.status.idle": "2023-05-01T12:25:31.683851Z",
          "shell.execute_reply.started": "2023-05-01T12:25:31.157921Z",
          "shell.execute_reply": "2023-05-01T12:25:31.682842Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "-1Ke95fAjMho",
        "outputId": "e5ce7c83-5b13-4f86-8dea-7b5e4b853ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f103bc3d5014>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prepare the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhasLabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-09e4676c80dd>\u001b[0m in \u001b[0;36mdata_preprocessing\u001b[0;34m(raw, hasLabel)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstart_pixel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mout_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mout_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.utils' has no attribute 'to_categorical'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split train in train and validation set\n",
        "\n",
        "We further split the train set in train and validation set. The validation set will be 10% from the original train set, therefore the split will be train/validation of 0.9/0.1."
      ],
      "metadata": {
        "_uuid": "8e25c569f0a0e817ca0462f3e3ea2f50feb480b0",
        "id": "edyz_xy3jMho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)"
      ],
      "metadata": {
        "_uuid": "36eb910dd0868a227a73c9759d6aee0a47ba2b1e",
        "execution": {
          "iopub.status.busy": "2023-05-01T12:25:39.285713Z",
          "iopub.execute_input": "2023-05-01T12:25:39.286052Z",
          "iopub.status.idle": "2023-05-01T12:25:40.120791Z",
          "shell.execute_reply.started": "2023-05-01T12:25:39.285988Z",
          "shell.execute_reply": "2023-05-01T12:25:40.119524Z"
        },
        "trusted": true,
        "id": "8X_a_Uk_jMho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dimmension of the processed train, validation and test set are as following:"
      ],
      "metadata": {
        "_uuid": "1e623ca7c0c61634c0e4b890fc5cf5cacb132552",
        "id": "8EvfR_e-jMho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MNIST train -  rows:\",X_train.shape[0],\" columns:\", X_train.shape[1:4])\n",
        "print(\"MNIST valid -  rows:\",X_val.shape[0],\" columns:\", X_val.shape[1:4])\n",
        "print(\"MNIST test -  rows:\",X_test.shape[0],\" columns:\", X_test.shape[1:4])"
      ],
      "metadata": {
        "_uuid": "98b3f1bf07e8ed614bf32b8e1955cbe90bff21c5",
        "execution": {
          "iopub.status.busy": "2023-05-01T12:25:43.669852Z",
          "iopub.execute_input": "2023-05-01T12:25:43.670153Z",
          "iopub.status.idle": "2023-05-01T12:25:43.677691Z",
          "shell.execute_reply.started": "2023-05-01T12:25:43.670103Z",
          "shell.execute_reply": "2023-05-01T12:25:43.676417Z"
        },
        "trusted": true,
        "id": "xu63xn5GjMho",
        "outputId": "062e2059-9c6e-4518-f560-09fa88188b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "MNIST train -  rows: 37800  columns: (28, 28, 1)\nMNIST valid -  rows: 4200  columns: (28, 28, 1)\nMNIST test -  rows: 28000  columns: (28, 28, 1)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the class imbalance for the resulted training set."
      ],
      "metadata": {
        "_uuid": "504b6e9a230b313c20feb5062aedaeac185cf973",
        "id": "wQuveOcvjMho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_count_per_class(yd):\n",
        "    ydf = pd.DataFrame(yd)\n",
        "    f, ax = plt.subplots(1,1, figsize=(12,4))\n",
        "    g = sns.countplot(ydf[0], order = np.arange(0,10))\n",
        "    g.set_title(\"Number of items for each class\")\n",
        "    g.set_xlabel(\"Category\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def get_count_per_class(yd):\n",
        "    ydf = pd.DataFrame(yd)\n",
        "    # Get the count for each label\n",
        "    label_counts = ydf[0].value_counts()\n",
        "\n",
        "    # Get total number of samples\n",
        "    total_samples = len(yd)\n",
        "\n",
        "\n",
        "    # Count the number of items in each class\n",
        "    for i in range(len(label_counts)):\n",
        "        label = label_counts.index[i]\n",
        "        count = label_counts.values[i]\n",
        "        percent = (count / total_samples) * 100\n",
        "        print(\"{}:   {} or {}%\".format(label, count, percent))\n",
        "\n",
        "plot_count_per_class(np.argmax(y_train,axis=1))\n",
        "get_count_per_class(np.argmax(y_train,axis=1))"
      ],
      "metadata": {
        "scrolled": true,
        "_uuid": "3d3c83a7be1be07cd4c3cacfaf8c667979a7d0ac",
        "trusted": true,
        "id": "XLc9SC22jMho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see also the class distribution of validation set."
      ],
      "metadata": {
        "_uuid": "926fa3b04b86032becd38bf2c5210ab17afbd2d2",
        "id": "RGvEc6WyjMhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_count_per_class(np.argmax(y_val,axis=1))\n",
        "get_count_per_class(np.argmax(y_val,axis=1))"
      ],
      "metadata": {
        "_uuid": "b99d881350de0bdf0ec9795a7b1f6013db5df5fd",
        "trusted": true,
        "id": "aeR6QwLNjMhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a id=\"52\">Train the model</a>\n",
        "\n",
        "### Build the model   \n",
        "\n",
        "\n",
        "\n",
        "We will use a **Sequential** model.\n",
        "* The **Sequential** model is a linear stack of layers. It can be first initialized and then we add layers using **add** method or we can add all layers at init stage. The layers added are as follows:\n",
        "\n",
        "* **Conv2D** is a 2D Convolutional layer (i.e. spatial convolution over images). The parameters used are:\n",
        " * filters - the number of filters (Kernels) used with this layer; here filters = 32;\n",
        " * kernel_size - the dimmension of the Kernel: (3 x 3);\n",
        " * activation - is the activation function used, in this case `relu`;\n",
        " * kernel_initializer - the function used for initializing the kernel;\n",
        " * input_shape - is the shape of the image presented to the CNN: in our case is 28 x 28\n",
        " The input and output of the **Conv2D** is a 4D tensor.\n",
        "* **Conv2D** with the following parameters:\n",
        " * filters: 32;\n",
        " * kernel_size: (3 x 3);\n",
        " * activation: `relu`;\n",
        "* **MaxPooling2D** is a Max pooling operation for spatial data. Parameters used here are:\n",
        " * *pool_size*, in this case (2,2), representing the factors by which to downscale in both directions;\n",
        "\n",
        "* **Dropout**. Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. The parameter used is:\n",
        " * *rate*, set here to 0.25.\n",
        "\n",
        "* **Conv2D** with the following parameters:\n",
        " * filters: 64;\n",
        " * kernel_size : (3 x 3);\n",
        " * activation : `relu`;\n",
        "\n",
        "* **MaxPooling2D** with parameter:\n",
        " * *pool_size* : (2,2);\n",
        "\n",
        "* **Dropout**. with parameter:\n",
        " * *rate* : 0.25;\n",
        "\n",
        "* **Conv2D** with the following parameters:\n",
        " * filters: 128;\n",
        " * kernel_size : (3 x 3);\n",
        " * activation : `relu`;\n",
        "\n",
        "* **Dropout**. with parameter:\n",
        " * *rate* : 0.4;\n",
        "\n",
        "* **Flatten**. This layer Flattens the input. Does not affect the batch size. It is used without parameters;\n",
        "\n",
        "* **Dense**. This layer is a regular fully-connected NN layer. It is used without parameters;\n",
        " * units - this is a positive integer, with the meaning: dimensionality of the output space; in this case is: 128;\n",
        " * activation - activation function : `relu`;\n",
        "\n",
        "* **Dropout**. with parameter:\n",
        " * *rate* : 0.3;\n",
        "\n",
        "* **Dense**. This is the final layer (fully connected). It is used with the parameters:\n",
        " * units: the number of classes (in our case 10);\n",
        " * activation : `softmax`; for this final layer it is used `softmax` activation (standard for multiclass classification)\n",
        "\n",
        "\n",
        "Then we compile the model, specifying as well the following parameters:\n",
        "* *loss*;\n",
        "* *optimizer*;\n",
        "* *metrics*.\n"
      ],
      "metadata": {
        "_uuid": "dd9405b5a3fe0c7bd5bebe0616190a8cf26d2811",
        "id": "fitTH-AkjMhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "model = Sequential()\n",
        "# Add convolution 2D\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu', padding=\"same\",\n",
        "        kernel_initializer='he_normal',input_shape=(IMG_ROWS, IMG_COLS, 1)))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32,kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "# Add dropouts to the model\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), strides=2,padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), strides=2,padding='same', activation='relu'))\n",
        "# Add dropouts to the model\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "# Add dropouts to the model\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(NUM_CLASSES, activation='softmax'))"
      ],
      "metadata": {
        "_uuid": "e31836cd5ec9d86340485404b8f613d1f574aca4",
        "execution": {
          "iopub.status.busy": "2023-05-01T12:26:26.849360Z",
          "iopub.execute_input": "2023-05-01T12:26:26.850336Z",
          "iopub.status.idle": "2023-05-01T12:26:27.380709Z",
          "shell.execute_reply.started": "2023-05-01T12:26:26.850265Z",
          "shell.execute_reply": "2023-05-01T12:26:27.379546Z"
        },
        "trusted": true,
        "id": "F7Xjb4i6jMhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile the model\n",
        "We then compile the model, with the layers and optimized defined."
      ],
      "metadata": {
        "_uuid": "58ab4b97c822b6712f80ddc9a39d13a582e57b78",
        "id": "NQohzbzJjMhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "_uuid": "17c6907aab80815ef8877f1c61a6796f9487f5fd",
        "trusted": true,
        "id": "aaYZBY-6jMhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspect the model\n",
        "\n",
        "Let's check the model we initialized."
      ],
      "metadata": {
        "_uuid": "ecb450d70539a62fb310bb7ed44849d2d01481ee",
        "id": "fEoyY67kjMhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "_uuid": "b4b923b11ceaf4a97677f8e24265e3e97ae1653b",
        "scrolled": true,
        "trusted": true,
        "id": "VQys-pLajMhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also visualize the model using model_plot."
      ],
      "metadata": {
        "_uuid": "d4c2f583b577f6fa9c143dfe13b3216751b85b06",
        "id": "a0hgUVsijMhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, to_file='model.png')\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
      ],
      "metadata": {
        "_uuid": "dd39462127ffd1c2d55ed0d15e613047bf6f91c6",
        "trusted": true,
        "id": "RbU0R9fwjMhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the model\n",
        "\n",
        "We run the model with the training set. We are also using the validation set (a subset from the orginal training set) for validation."
      ],
      "metadata": {
        "_uuid": "9265c2a5c2fbf1b028abc5fc82862b16eb9a4bbb",
        "id": "JLtfTKz7jMhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NO_EPOCHS = 10"
      ],
      "metadata": {
        "trusted": true,
        "id": "fWre34vwjMhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "earlystopper = EarlyStopping(monitor='loss', patience=PATIENCE, verbose=VERBOSE)\n",
        "checkpointer = ModelCheckpoint('best_model.h5',\n",
        "                                monitor='val_acc',\n",
        "                                verbose=VERBOSE,\n",
        "                                save_best_only=True,\n",
        "                                save_weights_only=True)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=NO_EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_val, y_val),\n",
        "          callbacks=[earlystopper, checkpointer])"
      ],
      "metadata": {
        "_uuid": "400494b7e0525069175625422e8c300bd7b41c51",
        "scrolled": true,
        "trusted": true,
        "id": "CGfIdshNjMhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## <a id=\"53\">Validation accuracy and  loss </a>\n",
        "\n",
        "\n",
        "We plot accuracy for validation set compared with the accuracy of training set, for each epoch, on the same graph. Then, we plot loss for validation set compared with the loss for training set.\n"
      ],
      "metadata": {
        "_uuid": "8699df44bc0a95f1a43a201d3dd566746d87173f",
        "id": "j2nHnGpgjMhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracy_and_loss(train_model):\n",
        "    hist = train_model.history\n",
        "    acc = hist['acc']\n",
        "    val_acc = hist['val_acc']\n",
        "    loss = hist['loss']\n",
        "    val_loss = hist['val_loss']\n",
        "    epochs = range(len(acc))\n",
        "    f, ax = plt.subplots(1,2, figsize=(14,6))\n",
        "    ax[0].plot(epochs, acc, 'g', label='Training accuracy')\n",
        "    ax[0].plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "    ax[0].set_title('Training and validation accuracy')\n",
        "    ax[0].legend()\n",
        "    ax[1].plot(epochs, loss, 'g', label='Training loss')\n",
        "    ax[1].plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    ax[1].set_title('Training and validation loss')\n",
        "    ax[1].legend()\n",
        "    plt.show()\n",
        "plot_accuracy_and_loss(history)"
      ],
      "metadata": {
        "_uuid": "b9e2bb7f25b02d491e34dd0d6d05943e287ae369",
        "trusted": true,
        "id": "DbFv61J7jMhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that the training is not overfitting, validation accuracy is not decreasing after a certain number of epochs. As well, the validation loss is not increasing after a certain number of epochs, as would have been expected in the case of overfitting. We achieved this by using the 3 Dropout layers inserted in our model. There are other strategies as well, for example by using a variable learning rate or data augmentation images. For the sake of simplicity and in order to keep the calculation very fast (the Kernel complete the training for 50 epochs in less than 10 min), we did not included these techniques for now."
      ],
      "metadata": {
        "_uuid": "e314698014d0f87ef04ea539bff653275b176060",
        "id": "xV527_IHjMhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a id=\"54\">Validation accuracy / class</a>\n",
        "\n",
        "Let's see in detail how well are the validation set classes predicted."
      ],
      "metadata": {
        "_uuid": "df9a25a0123b6fa2600ddb58805c96bb63463ef0",
        "id": "4cw9rPS9jMhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"run model - predict validation set\")\n",
        "score = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f'Last validation loss: {score[0]}, accuracy: {score[1]}')\n",
        "# load saved optimal model\n",
        "model_optimal = model\n",
        "model_optimal.load_weights('best_model.h5')\n",
        "score = model_optimal.evaluate(X_val, y_val, verbose=0)\n",
        "print(f'Best validation loss: {score[0]}, accuracy: {score[1]}')"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZY2AKo9MjMhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_show_classes(model, X_val, y_val):\n",
        "    #get the predictions for the test data\n",
        "    predicted_classes = model.predict_classes(X_val)\n",
        "    #get the indices to be plotted\n",
        "    y_true = np.argmax(y_val,axis=1)\n",
        "    correct = np.nonzero(predicted_classes==y_true)[0]\n",
        "    incorrect = np.nonzero(predicted_classes!=y_true)[0]\n",
        "    print(\"Correct predicted classes:\",correct.shape[0])\n",
        "    print(\"Incorrect predicted classes:\",incorrect.shape[0])\n",
        "    target_names = [\"Class {}:\".format(i) for i in range(NUM_CLASSES)]\n",
        "    print(classification_report(y_true, predicted_classes, target_names=target_names))\n",
        "    return correct, incorrect"
      ],
      "metadata": {
        "_uuid": "9bc726c2c275542447a2845ab856950f9930ba28",
        "trusted": true,
        "id": "IZBBOqHzjMhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We show the prediction precision, recall, f1-score for the validation set, with the last model."
      ],
      "metadata": {
        "id": "6dkYcXg1jMhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct, incorrect = predict_show_classes(model, X_val, y_val)"
      ],
      "metadata": {
        "_uuid": "3eed1fc17ccc4ab642121bbc6a3763eea61a1ec6",
        "trusted": true,
        "id": "TAcWjicWjMhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform the same operation using the optimal model."
      ],
      "metadata": {
        "id": "otxYE3-_jMhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct, incorrect =  predict_show_classes(model_optimal, X_val, y_val)"
      ],
      "metadata": {
        "_uuid": "9b902c728e50e405490b7898eb30728781197b48",
        "trusted": true,
        "id": "uD4MlriOjMhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation accuracy is above 0.99 (0.993) for most of the classes. Only class 4 has a lower accuracy, 0.98.\n",
        "\n",
        "Let's visualize few images from the validation set that were correctly classified (16 images)."
      ],
      "metadata": {
        "_uuid": "3696b5b116b4515c54a19563fd41ea2097439ecb",
        "id": "EVeUVanVjMhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(data_index,cmap=\"Blues\"):\n",
        "    # Plot the sample images now\n",
        "    f, ax = plt.subplots(4,4, figsize=(12,12))\n",
        "    y_true = np.argmax(y_val,axis=1)\n",
        "    for i, indx in enumerate(data_index[:16]):\n",
        "        ax[i//4, i%4].imshow(X_val[indx].reshape(IMG_ROWS,IMG_COLS), cmap=cmap)\n",
        "        ax[i//4, i%4].axis('off')\n",
        "        ax[i//4, i%4].set_title(\"True:{}  Pred:{}\".format(y_true[indx],predicted_classes[indx]))\n",
        "    plt.show()\n",
        "\n",
        "plot_images(correct, \"Greens\")"
      ],
      "metadata": {
        "_uuid": "70c882442d79ce5213de49ae9ecbd2dca6571f6d",
        "trusted": true,
        "id": "9GCRohvfjMhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the images from the validation set that were incorrecly classified (16 images)."
      ],
      "metadata": {
        "_uuid": "710193bee5cfa74a5744442fcd1dc4f163a685a6",
        "id": "qXuYeox_jMhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_images(incorrect, \"Reds\")"
      ],
      "metadata": {
        "_uuid": "becbd03e83ae8e01ee847f4629ad788a1d9f7416",
        "trusted": true,
        "id": "JMLBszCajMhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that most of the images from the validation data set  with classes incorrectly predicted were actually quite difficult to predict even by a human."
      ],
      "metadata": {
        "_uuid": "40820d0810e6f7ec9b5ccc472a1c124e2e8e3cd5",
        "id": "81r524kEjMhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a id=\"6\">Submission </a>\n",
        "\n",
        "Let's prepare now the submission.\n",
        "\n",
        "We predict first the classes for the test dataset."
      ],
      "metadata": {
        "_uuid": "bd82f76f90fb38f61411823e8941738727a8e3e0",
        "id": "pehACFhnjMhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_cat = model.predict(X_test, batch_size=64)"
      ],
      "metadata": {
        "_uuid": "81869e184b83c68ea26d9ebab36d92785d7498c9",
        "trusted": true,
        "id": "peG2pXq_jMhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We identify the predicted class for each image by selecting the column with the highest predicted value."
      ],
      "metadata": {
        "_uuid": "d5f0874368bb2f45a715d556455b4a2846ff4468",
        "id": "JwulBMNFjMhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(y_cat,axis=1)"
      ],
      "metadata": {
        "_uuid": "2e80c4b66a4680c9c8f4c67accad78bd7b927705",
        "trusted": true,
        "id": "WIA3L_T8jMhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the prediction in the output file."
      ],
      "metadata": {
        "_uuid": "fd17659b39518f4d107e30a81696a664035041b3",
        "trusted": true,
        "id": "0BNf24_cjMhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"submission.csv\"\n",
        "with open(output_file, 'w') as f :\n",
        "    f.write('ImageId,Label\\n')\n",
        "    for i in range(len(y_pred)) :\n",
        "        f.write(\"\".join([str(i+1),',',str(y_pred[i]),'\\n']))"
      ],
      "metadata": {
        "_uuid": "20ce6b60d75646fe96da9e00ac23b32e24c85527",
        "trusted": true,
        "id": "GBHvC9NQjMhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We repeat this using the optimal model."
      ],
      "metadata": {
        "id": "VPvylWBXjMht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_cat = model_optimal.predict(X_test, batch_size=64)\n",
        "y_pred = np.argmax(y_cat,axis=1)\n",
        "output_file = \"submission_optimal.csv\"\n",
        "with open(output_file, 'w') as f :\n",
        "    f.write('ImageId,Label\\n')\n",
        "    for i in range(len(y_pred)) :\n",
        "        f.write(\"\".join([str(i+1),',',str(y_pred[i]),'\\n']))"
      ],
      "metadata": {
        "trusted": true,
        "id": "qqSgojC9jMht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a id=\"7\">Conclusions</a>\n",
        "\n",
        "With a complex sequential model with multiple convolution layers and 50 epochs for the training, we obtained an accuracy  of approximatelly 0.993 for the validation set and of 0.99371 for the test set, after submission.\n"
      ],
      "metadata": {
        "_uuid": "e24053db4dbfec25fd1b90391586d7ce6bc32988",
        "id": "9sp6IMZzjMht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <a id=\"8\">References</a>\n",
        "\n",
        "[1] Yan LeCun, MNIST Database, http://yann.lecun.com/exdb/mnist/  \n",
        "[2] DanB, CollinMoris, Deep Learning From Scratch, https://www.kaggle.com/dansbecker/deep-learning-from-scratch  \n",
        "[3] DanB, Dropout and Strides for Larger Models, https://www.kaggle.com/dansbecker/dropout-and-strides-for-larger-models  \n",
        "[4] BGO, CNN with Keras, https://www.kaggle.com/bugraokcu/cnn-with-keras    \n",
        "\n"
      ],
      "metadata": {
        "_uuid": "1b45e3422fb9507da66ce7af67a97b54e7fa7683",
        "id": "K_8MlFqSjMht"
      }
    }
  ]
}